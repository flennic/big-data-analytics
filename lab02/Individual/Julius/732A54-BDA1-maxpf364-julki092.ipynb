{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: All code snippets were executed on Heffa/hdsf and are hence commented out here. Time requirements were mentioned for the first assigment (since we were asked about it) but not for the other assignments (since we were not asked for those).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxilary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def print_result(path, colnames, separator = \" \", replace = True):\n",
    "    \n",
    "    df = pd.read_csv(path, sep = separator, header = None)\n",
    "    df.columns = colnames\n",
    "\n",
    "    if replace == True: \n",
    "        df.replace(',','',regex=True,inplace=True)\n",
    "        df.replace('\\(','',regex=True,inplace=True)\n",
    "        df.replace('\\)','',regex=True,inplace=True)\n",
    "\n",
    "    print(df.head(10))\n",
    "    print(\"...\")\n",
    "    print(df.tail(10))\n",
    "    \n",
    "    return(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instructions are slightly unclear. They are understood in the following way:\n",
    "\n",
    "1. temperature-readings.csv with spark (only temperature)\n",
    "2. temperature-readings.csv with spark (temperature + station number)\n",
    "3. temperatures-big.csv without spark (only temperature)\n",
    "4. temperatures-big.csv with spark (only temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 temperature-readings.csv with spark (only temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time: Ca. 2 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pyspark\n",
    "\n",
    "# Start SparkContext\n",
    "sc = pyspark.SparkContext(appName=\"BDA\")\n",
    "\n",
    "# Import data as pyspark.rdd.RDD\n",
    "directory = \"/user/x_julki/data/temperature-readings.csv\"\n",
    "# directory = \"./input/temperature-readings_100000.csv\"\n",
    "res = sc.textFile(directory)\n",
    "\n",
    "# Split every line l by ;\n",
    "res = res.map(lambda l: l.split(\";\"))\n",
    "\n",
    "# Convert every line to tuples in the form (year, temperature) with type (int, (float))\n",
    "res = res.map(lambda l: (int(l[1][0:4]), float(l[3])))\n",
    "\n",
    "# Filter to get lines in the year range [1950, 2014]\n",
    "res = res.filter(lambda l: l[0] >= 1950 and l[0] <= 2014)\n",
    "\n",
    "# Get maximum temperatures by year\n",
    "res_max = res.reduceByKey(max)\n",
    "\n",
    "# Get minimum temperatures by year\n",
    "res_min = res.reduceByKey(min)\n",
    "\n",
    "res = res_max.zip(res_min).map(lambda l: (l[0][0], l[0][1], l[1][1]))\n",
    "res = res.sortBy(lambda l: l[1], ascending=False, numPartitions=1)\n",
    "\n",
    "res.saveAsTextFile(\"BDA1_results/Asg1_withSpark_temperature-readings_temperatureOnly.txt\")\n",
    "# res.saveAsTextFile(\"BDA1_results_100000/Asg1_withSpark_temperature-readings_temperatureOnly.txt\")\n",
    "\n",
    "# Stop SparkSession\n",
    "sc.stop()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year MaxTemp MinTemp\n",
      "0  1975    36.1   -37.0\n",
      "1  1992    35.4   -36.1\n",
      "2  1994    34.7   -40.5\n",
      "3  2010    34.4   -41.7\n",
      "4  2014    34.4   -42.5\n",
      "5  1989    33.9   -38.2\n",
      "6  1982    33.8   -42.2\n",
      "7  1968    33.7   -42.0\n",
      "8  1966    33.5   -49.4\n",
      "9  2002    33.3   -42.2\n",
      "...\n",
      "    Year MaxTemp MinTemp\n",
      "55  1981    29.7   -44.0\n",
      "56  1987    29.6   -47.3\n",
      "57  1984    29.5   -39.2\n",
      "58  1967    29.5   -45.4\n",
      "59  1960    29.4   -40.0\n",
      "60  1950    29.4   -42.0\n",
      "61  1998    29.2   -42.7\n",
      "62  1965    28.5   -44.0\n",
      "63  1951    28.5   -42.0\n",
      "64  1962    27.4   -42.0\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "path = \"output/BDA1_results_final/Asg1_withSpark_temperature-readings_temperatureOnly.txt/part-00000\"\n",
    "colnames = [\"Year\", \"MaxTemp\", \"MinTemp\"]\n",
    "print_result(path, colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 temperature-readings.csv with spark (temperature + station number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time: Ca. 2 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pyspark\n",
    "\n",
    "# Start SparkContext\n",
    "sc = pyspark.SparkContext(appName=\"BDA\")\n",
    "\n",
    "# Import data as pyspark.rdd.RDD\n",
    "directory = \"/user/x_julki/data/temperature-readings.csv\"\n",
    "# directory = \"./input/temperature-readings_100000.csv\"\n",
    "res = sc.textFile(directory)\n",
    "\n",
    "# Split every line l by ;\n",
    "res = res.map(lambda l: l.split(\";\"))\n",
    "\n",
    "# Convert every line to tuples in the form (year, (temperature, stationnumber)) with type (int, (float, int))\n",
    "res = res.map(lambda l: (int(l[1][0:4]), (float(l[3]), int(l[0]))))\n",
    "\n",
    "# Filter to get lines in the year range [1950, 2014]\n",
    "res = res.filter(lambda l: l[0] >= 1950 and l[0] <= 2014)\n",
    "\n",
    "# Get maximum temperatures by year\n",
    "res_max = res.reduceByKey(lambda x, y: (x if x[0] > y[0] else y))\n",
    "\n",
    "# Get minimum temperatures by year\n",
    "res_min = res.reduceByKey(lambda x, y: (x if x[0] < y[0] else y))\n",
    "\n",
    "res = res_max.zip(res_min).map(lambda l: (l[0][0], l[0][1][0], l[1][1][0], l[1][1][1]))\n",
    "res = res.sortBy(lambda l: l[1], ascending=False, numPartitions=1)\n",
    "\n",
    "res.saveAsTextFile(\"BDA1_results/Asg1_withSpark_temperature-readings_temperaturePlusStationnumber.txt\")\n",
    "# res.saveAsTextFile(\"BDA1_results_100000/Asg1_withSpark_temperature-readings_temperaturePlusStationnumber.txt\")\n",
    "\n",
    "# Stop SparkSession\n",
    "sc.stop()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year MaxTemp MinTemp Stationnumber\n",
      "0  1975    36.1   -37.0        157860\n",
      "1  1992    35.4   -36.1        179960\n",
      "2  1994    34.7   -40.5        179960\n",
      "3  2010    34.4   -41.7        191910\n",
      "4  2014    34.4   -42.5        192840\n",
      "5  1989    33.9   -38.2        166870\n",
      "6  1982    33.8   -42.2        113410\n",
      "7  1968    33.7   -42.0        179950\n",
      "8  1966    33.5   -49.4        179950\n",
      "9  2002    33.3   -42.2        169860\n",
      "...\n",
      "    Year MaxTemp MinTemp Stationnumber\n",
      "55  1981    29.7   -44.0        166870\n",
      "56  1987    29.6   -47.3        123480\n",
      "57  1984    29.5   -39.2        123480\n",
      "58  1967    29.5   -45.4        166870\n",
      "59  1960    29.4   -40.0        167710\n",
      "60  1950    29.4   -42.0        155910\n",
      "61  1998    29.2   -42.7        169860\n",
      "62  1965    28.5   -44.0        189780\n",
      "63  1951    28.5   -42.0        155910\n",
      "64  1962    27.4   -42.0        181900\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "path = \"output/BDA1_results_final/Asg1_withSpark_temperature-readings_temperaturePlusStationnumber.txt/part-00000\"\n",
    "colnames = [\"Year\", \"MaxTemp\", \"MinTemp\", \"Stationnumber\"]\n",
    "print_result(path, colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 temperatures-big.csv without spark (only temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time: Ca. 49 min (start at ca. 11:10:40 to 2019-04-10 11:59:40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import csv \n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Start timer\n",
    "start = time.time()\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "# Import data\n",
    "# directory = \"./input/temperature-readings_100000.csv\" # locally\n",
    "directory = \"../../hadoop_examples/shared_data/temperatures-big.csv\"\n",
    "\n",
    "# Identify the max and min temperatures by year\n",
    "cnt = 0\n",
    "\n",
    "with open(directory, \"r\") as input_file:\n",
    " \n",
    "    input_file = csv.reader(input_file, delimiter = ';')\n",
    "    input_dict = {}\n",
    "    \n",
    "    for row in input_file: \n",
    "\n",
    "        cnt += 1\n",
    "        if cnt % 100000 == 0:\n",
    "            print(\"{} | {}\".format(datetime.datetime.now(), cnt))\n",
    "\n",
    "        key = int(row[1][0:4])\n",
    "        temp = float(row[3])\n",
    "\n",
    "        if key >= 1950 and key <= 2014:\n",
    "            if key not in input_dict.keys():\n",
    "                input_dict.update({key: {'max': temp, 'min': temp}}) \n",
    "            else:\n",
    "                if input_dict[key]['max'] < temp:\n",
    "                    input_dict[key]['max'] = temp\n",
    "                elif input_dict[key]['min'] > temp:\n",
    "                    input_dict[key]['min'] = temp\n",
    "                \n",
    "                \n",
    "    res = sorted([(k, v['max'], v['min']) for k, v in input_dict.items()], key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "# Output min result as .csv file \n",
    "# with open(\"BDA1_results_100000/Asg1_withoutSpark_temperatures-big_temperatureOnly.txt\", \"w+\") as output_file:\n",
    "with open(\"BDA1_results/Asg1_withoutSpark_temperatures-big_temperatureOnly.txt\", \"w+\") as output_file:\n",
    "\n",
    "    output_file = csv.writer(output_file)    \n",
    "    \n",
    "    print(\"Writing to .txt now\")\n",
    "    for element in res:\n",
    "        output_file.writerow(element)\n",
    "\n",
    "# Stop timer\n",
    "end = time.time()\n",
    "print(datetime.datetime.now())\n",
    "print(str(end - start) + \" seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  MaxTemp  MinTemp\n",
      "0  1975     36.1    -37.0\n",
      "1  1992     35.4    -36.1\n",
      "2  1994     34.7    -40.5\n",
      "3  2010     34.4    -41.7\n",
      "4  2014     34.4    -42.5\n",
      "5  1989     33.9    -38.2\n",
      "6  1982     33.8    -42.2\n",
      "7  1968     33.7    -42.0\n",
      "8  1966     33.5    -49.4\n",
      "9  1983     33.3    -38.2\n",
      "...\n",
      "    Year  MaxTemp  MinTemp\n",
      "55  1993     29.7    -39.0\n",
      "56  1987     29.6    -47.3\n",
      "57  1967     29.5    -45.4\n",
      "58  1984     29.5    -39.2\n",
      "59  1950     29.4    -42.0\n",
      "60  1960     29.4    -40.0\n",
      "61  1998     29.2    -42.7\n",
      "62  1951     28.5    -42.0\n",
      "63  1965     28.5    -44.0\n",
      "64  1962     27.4    -42.0\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "path = \"output/BDA1_results_final/Asg1_withoutSpark_temperatures-big_temperatureOnly.txt\"\n",
    "colnames = [\"Year\", \"MaxTemp\", \"MinTemp\"]\n",
    "print_result(path, colnames, separator = \",\", replace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 temperatures-big.csv with spark (only temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time: Ca. 8 min (start at 00:07:23, end at 00:15:10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pyspark\n",
    "\n",
    "# Start SparkContext\n",
    "sc = pyspark.SparkContext(appName=\"BDA\")\n",
    "\n",
    "# Import data as pyspark.rdd.RDD\n",
    "directory = \"/user/common/732A54/temperatures-big.csv\"\n",
    "# directory = \"./input/temperature-readings_10000.csv\"\n",
    "res = sc.textFile(directory)\n",
    "\n",
    "# Split every line l by ;\n",
    "res = res.map(lambda l: l.split(\";\"))\n",
    "\n",
    "# Convert every line to tuples in the form (year, temperature) with type (int, (float))\n",
    "res = res.map(lambda l: (int(l[1][0:4]), float(l[3])))\n",
    "\n",
    "# Filter to get lines in the year range [1950, 2014]\n",
    "res = res.filter(lambda l: l[0] >= 1950 and l[0] <= 2014)\n",
    "\n",
    "# Get maximum temperatures by year\n",
    "res_max = res.reduceByKey(max)\n",
    "\n",
    "# Get minimum temperatures by year\n",
    "res_min = res.reduceByKey(min)\n",
    "\n",
    "res = res_max.zip(res_min).map(lambda l: (l[0][0], l[0][1], l[1][1]))\n",
    "res = res.sortBy(lambda l: l[1], ascending=False, numPartitions=1)\n",
    "\n",
    "res.saveAsTextFile(\"BDA1_results/Asg1_withSpark_temperatures-big_temperatureOnly.txt\")\n",
    "# res.saveAsTextFile(\"BDA1_results_100000/Asg1_withSpark_temperature-readings_temperatureOnly.txt\")\n",
    "\n",
    "# Stop SparkSession\n",
    "sc.stop()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year MaxTemp MinTemp\n",
      "0  1975    36.1   -37.0\n",
      "1  1992    35.4   -36.1\n",
      "2  1994    34.7   -40.5\n",
      "3  2010    34.4   -41.7\n",
      "4  2014    34.4   -42.5\n",
      "5  1989    33.9   -38.2\n",
      "6  1982    33.8   -42.2\n",
      "7  1968    33.7   -42.0\n",
      "8  1966    33.5   -49.4\n",
      "9  1983    33.3   -38.2\n",
      "...\n",
      "    Year MaxTemp MinTemp\n",
      "55  1993    29.7   -39.0\n",
      "56  1987    29.6   -47.3\n",
      "57  1967    29.5   -45.4\n",
      "58  1984    29.5   -39.2\n",
      "59  1950    29.4   -42.0\n",
      "60  1960    29.4   -40.0\n",
      "61  1998    29.2   -42.7\n",
      "62  1951    28.5   -42.0\n",
      "63  1965    28.5   -44.0\n",
      "64  1962    27.4   -42.0\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "path = \"output/BDA1_results_final/Asg1_withSpark_temperatures-big_temperatureOnly.txt/part-00000\"\n",
    "colnames = [\"Year\", \"MaxTemp\", \"MinTemp\"]\n",
    "print_result(path, colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER: Obviously, the version without spark took much longer (ca. 40 minutes longer) than the version with spark. This is because all computations and IO is done sequentially and not in parallel with help of HDFS and spark.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 temperature-readings.csv with spark (duplicate stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pyspark\n",
    "\n",
    "# Start SparkContext\n",
    "sc = pyspark.SparkContext(appName=\"BDA\")\n",
    "\n",
    "# Import data as pyspark.rdd.RDD\n",
    "directory = \"/user/x_julki/data/temperature-readings.csv\"\n",
    "# directory = \"./input/temperature-readings_100000.csv\"\n",
    "res = sc.textFile(directory)\n",
    "\n",
    "# Split every line l by ;\n",
    "res = res.map(lambda l: l.split(\";\"))\n",
    "\n",
    "# Convert every line to tuples\n",
    "res = res.map(lambda l: ((int(l[1][0:4]), int(l[1][5:7])), (float(l[3]))))\n",
    "\n",
    "# Filter to get lines in the year range [1950, 2014] AND temperature values > 10\n",
    "res = res.filter(lambda l: l[0][0] >= 1950 and l[0][0] <= 2014)\n",
    "\n",
    "# Map to reduce the temperature from the values\n",
    "res = res.map(lambda l: ((l[0][0], l[0][1]), (1 if l[1] > 10 else 0)))\n",
    "\n",
    "# Count number of readings with temperatures > 10\n",
    "res = res.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Sort by year and month (not required but nice for output)\n",
    "res = res.sortByKey(ascending = False, numPartitions = 1)\n",
    "\n",
    "res.saveAsTextFile(\"BDA1_results/Asg2_withSpark_temperature-readings_duplicateStations.txt\")\n",
    "# res.saveAsTextFile(\"BDA1_results_100000/Asg2_withSpark_temperature-readings_duplicateStations.txt\")\n",
    "\n",
    "# Stop SparkSession\n",
    "sc.stop()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year Month   Count\n",
      "0  2014    12       3\n",
      "1  2014    11    8139\n",
      "2  2014    10   42191\n",
      "3  2014     9   86090\n",
      "4  2014     8  124045\n",
      "5  2014     7  147681\n",
      "6  2014     6  101711\n",
      "7  2014     5   57250\n",
      "8  2014     4   19862\n",
      "9  2014     3    4213\n",
      "...\n",
      "     Year Month Count\n",
      "770  1950    10  1248\n",
      "771  1950     9  3612\n",
      "772  1950     8  5954\n",
      "773  1950     7  5811\n",
      "774  1950     6  4886\n",
      "775  1950     5  2802\n",
      "776  1950     4   352\n",
      "777  1950     3    81\n",
      "778  1950     2     0\n",
      "779  1950     1     0\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "path = \"output/BDA1_results_final/Asg2_withSpark_temperature-readings_duplicateStations.txt/part-00000\"\n",
    "colnames = [\"Year\", \"Month\", \"Count\"]\n",
    "\n",
    "print_result(path, colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 temperature-readings.csv with spark (unique stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pyspark\n",
    "\n",
    "# Start SparkContext\n",
    "sc = pyspark.SparkContext(appName=\"BDA\")\n",
    "\n",
    "# Import data as pyspark.rdd.RDD\n",
    "directory = \"/user/x_julki/data/temperature-readings.csv\"\n",
    "# directory = \"./input/temperature-readings_100000.csv\"\n",
    "res = sc.textFile(directory)\n",
    "\n",
    "# Split every line l by ;\n",
    "res = res.map(lambda l: l.split(\";\"))\n",
    "\n",
    "# Convert every line to tuples\n",
    "res = res.map(lambda l: ((int(l[1][0:4]), int(l[1][5:7]), int(l[0])), (float(l[3]))))\n",
    "\n",
    "# Filter to get lines in the year range [1950, 2014]\n",
    "res = res.filter(lambda l: l[0][0] >= 1950 and l[0][0] <= 2014)\n",
    "\n",
    "# Map to get a constant of value 1 for keys with temperature values > 10 (else 0)\n",
    "res = res.map(lambda l: ((l[0][0], l[0][1], l[0][2]), (1 if l[1] > 10 else 0)))\n",
    "\n",
    "# Reduce to assign value 1 to keys with temperature values > 10 (else 0)\n",
    "res = res.reduceByKey(lambda x, y: 1 if x + y >= 1 else 0)\n",
    "\n",
    "# Map to remove the station from the key\n",
    "res = res.map(lambda l: ((l[0][0], l[0][1]), l[1]))\n",
    "\n",
    "# Count number of readings with temperatures > 10\n",
    "res = res.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Sort by year and month (not required but nice for output)\n",
    "res = res.sortByKey(ascending = False, numPartitions = 1)\n",
    "print(res.collect())\n",
    "              \n",
    "res.saveAsTextFile(\"BDA1_results/Asg2_withSpark_temperature-readings_uniqueStations.txt\")\n",
    "# res.saveAsTextFile(\"BDA1_results_100000/Asg2_withSpark_temperature-readings_uniqueStations.txt\")\n",
    "\n",
    "# Stop SparkSession\n",
    "sc.stop()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year Month Count\n",
      "0  2014    12     1\n",
      "1  2014    11   158\n",
      "2  2014    10   270\n",
      "3  2014     9   296\n",
      "4  2014     8   296\n",
      "5  2014     7   297\n",
      "6  2014     6   298\n",
      "7  2014     5   296\n",
      "8  2014     4   254\n",
      "9  2014     3   169\n",
      "...\n",
      "     Year Month Count\n",
      "770  1950    10    46\n",
      "771  1950     9    50\n",
      "772  1950     8    49\n",
      "773  1950     7    49\n",
      "774  1950     6    47\n",
      "775  1950     5    46\n",
      "776  1950     4    36\n",
      "777  1950     3    26\n",
      "778  1950     2     0\n",
      "779  1950     1     0\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "path = \"output/BDA1_results_final/Asg2_withSpark_temperature-readings_uniqueStations.txt/part-00000\"\n",
    "colnames = [\"Year\", \"Month\", \"Count\"]\n",
    "\n",
    "print_result(path, colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pyspark\n",
    "\n",
    "# Start SparkContext\n",
    "sc = pyspark.SparkContext(appName=\"BDA\")\n",
    "\n",
    "# Import data as pyspark.rdd.RDD\n",
    "directory = \"/user/x_julki/data/temperature-readings.csv\"\n",
    "# directory = \"./input/temperature-readings_100000.csv\"\n",
    "res = sc.textFile(directory)\n",
    "\n",
    "# Split every line l by ;\n",
    "res = res.map(lambda l: l.split(\";\"))\n",
    "\n",
    "# Convert every line to tuples\n",
    "res = res.map(lambda l: ((int(l[1][0:4]), int(l[1][5:7]), int(l[1][8:10]), int(l[0])), (float(l[3]), float(l[3]))))\n",
    "\n",
    "# Filter to get lines in the year range [1960, 2014]\n",
    "res = res.filter(lambda l: l[0][0] >= 1960 and l[0][0] <= 2014)\n",
    "\n",
    "# Reduce to get the min and max temperature for each (year, month, date, station) key\n",
    "res = res.reduceByKey(lambda x, y: ((min(x[0], y[0]), max(x[1], y[1]))))\n",
    "\n",
    "# Map to remove date, to add constant 1 for counting, to compute the sum of min and max for each (year, month, date, station) key\n",
    "res = res.map(lambda l: ((l[0][0], l[0][1], l[0][3]), (2, sum(l[1]))))\n",
    "\n",
    "# Reduce and map to compute average for each (year, month, station) key across all days\n",
    "res = res.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).map(lambda l: ((l[0][0], l[0][1], l[0][2]), (round(l[1][1]/l[1][0], 2))))\n",
    "\n",
    "# Sort by year and month (not required but nice for output)\n",
    "res = res.sortByKey(ascending = False, numPartitions = 1)\n",
    "\n",
    "res.saveAsTextFile(\"BDA1_results/Asg3_withSpark_temperature-readings.txt\")\n",
    "# res.saveAsTextFile(\"BDA1_results_100000/Asg3_withSpark_temperature-readings.txt\")\n",
    "\n",
    "# Stop SparkSession\n",
    "sc.stop()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year Month Station Average\n",
      "0  2014    12  192840  -11.83\n",
      "1  2014    12  191910  -11.55\n",
      "2  2014    12  191720  -11.87\n",
      "3  2014    12  189720  -10.09\n",
      "4  2014    12  188850   -7.56\n",
      "5  2014    12  188820   -7.47\n",
      "6  2014    12  188800   -6.87\n",
      "7  2014    12  183750    -9.6\n",
      "8  2014    12  182930   -9.77\n",
      "9  2014    12  182910   -9.97\n",
      "...\n",
      "        Year Month Station Average\n",
      "215043  1960     1   62190   -0.65\n",
      "215044  1960     1   62180   -1.76\n",
      "215045  1960     1   55570   -0.06\n",
      "215046  1960     1   54330    0.23\n",
      "215047  1960     1   53650   -1.95\n",
      "215048  1960     1   53370   -0.44\n",
      "215049  1960     1   53360    0.39\n",
      "215050  1960     1   53260   -0.84\n",
      "215051  1960     1   53200   -0.72\n",
      "215052  1960     1   52230   -0.17\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "path = \"output/BDA1_results_final/Asg3_withSpark_temperature-readings.txt/part-00000\"\n",
    "colnames = [\"Year\", \"Month\", \"Station\", \"Average\"]\n",
    "\n",
    "print_result(path, colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pyspark\n",
    "\n",
    "# Start SparkContext\n",
    "sc = pyspark.SparkContext(appName=\"BDA\")\n",
    "\n",
    "# Preparing stations w.r.t temperature --------------------------------------------------------\n",
    "\n",
    "# Import data as pyspark.rdd.RDD\n",
    "directory = \"/user/x_julki/data/temperature-readings.csv\"\n",
    "# directory = \"./input/temperature-readings_100000.csv\"\n",
    "res = sc.textFile(directory)\n",
    "\n",
    "# Split every line l by ;\n",
    "res = res.map(lambda l: l.split(\";\"))\n",
    "\n",
    "# Map to convert every line to tuples in the form (stationnumber, temperature) with type (int, float)\n",
    "res = res.map(lambda l: (int(l[0]), float(l[3]))) \n",
    "\n",
    "# Reduce to get the maximum measurement by stationnumber\n",
    "res = res.reduceByKey(lambda x, y: (max(x, y)))\n",
    "\n",
    "# Filter to only get stations with max temp. between 25 and 30 degrees\n",
    "temp = res.filter(lambda l: l[1] >= 25 and l[1] <= 30)\n",
    "\n",
    "# Preparing stations w.r.t precipitation --------------------------------------------------------\n",
    "\n",
    "# Import data as pyspark.rdd.RDD\n",
    "directory = \"/user/x_julki/data/precipitation-readings.csv\"\n",
    "# directory = \"./input/precipitation-readings_100000.csv\"\n",
    "res = sc.textFile(directory)\n",
    "\n",
    "# Split every line l by ;\n",
    "res = res.map(lambda l: l.split(\";\"))\n",
    "\n",
    "# Map to convert every line to tuples in the form (stationnumber, precipitation) with type (int, float)\n",
    "res = res.map(lambda l: (int(l[0]), float(l[3])))\n",
    "\n",
    "# Reduce to get the maximum measurement by stationnumber\n",
    "res = res.reduceByKey(lambda x, y: (max(x, y)))\n",
    "\n",
    "# Filter to only get stations with max precipitation between 100 and 200\n",
    "prec = res.filter(lambda l: l[1] >= 100 and l[1] <= 200)\n",
    "\n",
    "# Join and prepare output -----------------------------------------------------------------------\n",
    "\n",
    "# Join by stationnumber\n",
    "res = temp.join(prec)\n",
    "\n",
    "# Sort by stationnumber (not required but nice for output)\n",
    "res = res.sortByKey(ascending = False, numPartitions = 1)\n",
    "\n",
    "res.saveAsTextFile(\"BDA1_results/Asg4_withSpark_temperature-readings_precipitation-readings.txt\")\n",
    "# res.saveAsTextFile(\"BDA1_results_100000/Asg4_withSpark_temperature-readings_precipitation-readings.txt\")\n",
    "\n",
    "sc.stop()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The file is empty. There are no stations that meet the requirements.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pyspark\n",
    "\n",
    "# Start SparkContext\n",
    "sc = pyspark.SparkContext(appName=\"BDA\")\n",
    "\n",
    "# Preparing stations w.r.t Ostergotland list ----------------------------------------------------\n",
    "\n",
    "# Import data as pyspark.rdd.RDD\n",
    "directory = \"/user/x_julki/data/stations-Ostergotland.csv\"\n",
    "# directory = \"./input/stations-Ostergotland.csv\"\n",
    "res = sc.textFile(directory)\n",
    "\n",
    "# Split every line l by ;\n",
    "res = res.map(lambda l: l.split(\";\"))\n",
    "\n",
    "# Obtain a set with all station numbers from \n",
    "stat = res.map(lambda l: (int(l[0])))\n",
    "stat = set(stat.collect())\n",
    "\n",
    "# Preparing stations w.r.t precipitation --------------------------------------------------------\n",
    "\n",
    "# Import data as pyspark.rdd.RDD\n",
    "directory = \"/user/x_julki/data/precipitation-readings.csv\"\n",
    "# directory = \"./input/precipitation-readings_100000.csv\"\n",
    "res = sc.textFile(directory)\n",
    "\n",
    "# Map to split every line l by ;\n",
    "res = res.map(lambda l: l.split(\";\"))\n",
    "\n",
    "# Map to convert every line to tuples in the form ((year, month, stationnumber), (precipitation))\n",
    "res = res.map(lambda l: ((int(l[1][0:4]), int(l[1][5:7]), int(l[0])), (float(l[3]))))\n",
    "\n",
    "# Filter so that we only have Ostergotland stations and only measurements for 1993 to 2016\n",
    "res = res.filter(lambda l: l[0][2] in stat and l[0][0] >= 1993 and l[0][0] <= 2016)\n",
    "\n",
    "# Conduct the computations ----------------------------------------------------------------------\n",
    "\n",
    "# Reduce to sum up the values across all (year, month, stationnumber) keys\n",
    "res = res.reduceByKey(lambda x, y: (x + y))\n",
    "\n",
    "# Map to remove stationnumber from key \n",
    "res = res.map(lambda l: ((l[0][0], l[0][1]), (1, l[1])))\n",
    "\n",
    "# Reduce to sum up the values across all (year, month) keys\n",
    "res = res.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "# Map to compute the average\n",
    "res = res.map(lambda l: ((l[0][0], l[0][1]), (round(l[1][1]/l[1][0], 2))))\n",
    "\n",
    "# Sort by year and month (not required but nice for output)\n",
    "res = res.sortByKey(ascending = False, numPartitions = 1)\n",
    "\n",
    "res.saveAsTextFile(\"BDA1_results/Asg5_withSpark_precipitation-readings_stations-Ostergotland.txt\")\n",
    "# res.saveAsTextFile(\"BDA1_results_100000/Asg5_withSpark_precipitation-readings_stations-Ostergotland.txt\")\n",
    "\n",
    "sc.stop()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year Month Total monthly precipitation (averaged across stations)\n",
      "0  2016     7                                                0.0    \n",
      "1  2016     6                                              47.66    \n",
      "2  2016     5                                              29.25    \n",
      "3  2016     4                                               26.9    \n",
      "4  2016     3                                              19.96    \n",
      "5  2016     2                                              21.56    \n",
      "6  2016     1                                              22.33    \n",
      "7  2015    12                                              28.93    \n",
      "8  2015    11                                              63.89    \n",
      "9  2015    10                                               2.26    \n",
      "...\n",
      "     Year Month Total monthly precipitation (averaged across stations)\n",
      "270  1994     1                                               22.1    \n",
      "271  1993    12                                               37.1    \n",
      "272  1993    11                                               42.8    \n",
      "273  1993    10                                               43.2    \n",
      "274  1993     9                                               40.6    \n",
      "275  1993     8                                               80.7    \n",
      "276  1993     7                                               95.4    \n",
      "277  1993     6                                               56.5    \n",
      "278  1993     5                                               21.1    \n",
      "279  1993     4                                                0.0    \n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "path = \"output/BDA1_results_final/Asg5_withSpark_precipitation-readings_stations-Ostergotland.txt/part-00000\"\n",
    "colnames = [\"Year\", \"Month\", \"Total monthly precipitation (averaged across stations)\"]\n",
    "\n",
    "print_result(path, colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to obtain all precipitation for the Ostergotland stations (for local testing)\n",
    "# directory = \"./input/precipitation-readings.csv\"\n",
    "# df = pd.read_csv(directory, sep = \";\", header = None)\n",
    "# df.columns = [\"station\", \"date\", \"time\", \"val\", \"other\"]\n",
    "# print(df.head())\n",
    "\n",
    "# directory = \"./input/stations-Ostergotland.csv\"\n",
    "# stat = pd.read_csv(directory, sep = \";\", header = None)\n",
    "# stat = set(stat.iloc[:,0])\n",
    "# print(len(stat))\n",
    "\n",
    "# df_sub = df[df['station'].isin(stat)]\n",
    "# df_sub.to_csv(\"./input/precipitation-readings_Ostergotland.csv\", header = False, sep = \";\", index = False)\n",
    "# print(df_sub.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In red: code run on heffa (see printed results).\n",
    "- In green: code for testing locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pyspark\n",
    "\n",
    "# Start SparkContext\n",
    "sc = pyspark.SparkContext(appName=\"BDA\")\n",
    "\n",
    "# Preparing stations w.r.t Ostergotland list ----------------------------------------------------\n",
    "\n",
    "# Import data as pyspark.rdd.RDD\n",
    "directory = \"/user/x_julki/data/stations-Ostergotland.csv\"\n",
    "# directory = \"./input/stations-Ostergotland.csv\"\n",
    "res = sc.textFile(directory)\n",
    "\n",
    "# Split every line l by ;\n",
    "res = res.map(lambda l: l.split(\";\"))\n",
    "\n",
    "# Obtain a set with all station numbers from \n",
    "stat = res.map(lambda l: (int(l[0])))\n",
    "stat = set(stat.collect())\n",
    "\n",
    "# Preparing stations w.r.t temperature (focus long-term monthly averages 1950-1980) -------------\n",
    "\n",
    "# Import data as pyspark.rdd.RDD\n",
    "directory = \"/user/x_julki/data/temperature-readings.csv\"\n",
    "# directory = \"./input/temperature-readings_100000.csv\"\n",
    "res = sc.textFile(directory)\n",
    "\n",
    "# Map to split every line l by ;\n",
    "res = res.map(lambda l: l.split(\";\"))\n",
    "\n",
    "# Map to convert every line to tuples in the form ((year, month, day, stationnumber), (temperature, temperature))\n",
    "res = res.map(lambda l: ((int(l[1][0:4]), int(l[1][5:7]), int(l[1][8:10]), int(l[0])), (float(l[3]), float(l[3]))))\n",
    "\n",
    "# Filter so that we only have measurements for 1950 to 1980\n",
    "res = res.filter(lambda l: l[0][0] >= 1950 and l[0][0] <= 1980)\n",
    "\n",
    "# Reduce to get the min and max values for all (year, month, day, stationnumber) keys\n",
    "res = res.reduceByKey(lambda x, y: (max(x[0], y[0]), min(x[1], y[1])))\n",
    "\n",
    "# Map to sum the min and max values across all (year, month, day, stationnumber) keys, and remove day\n",
    "res = res.map(lambda l: ((l[0][0], l[0][1], l[0][3]), (2, sum(l[1]))))\n",
    "\n",
    "# Reduce to sum up the values across all (year, month, stationnumber) keys\n",
    "res = res.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "# Map to compute the averages for all (year, month, stationnumber) keys, and remove stationnumber\n",
    "res = res.map(lambda l: ((l[0][0], l[0][1]), (1, l[1][1]/l[1][0])))\n",
    "\n",
    "# Reduce to sum up the values across all (year, month) keys\n",
    "res = res.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "# Map to compute the averages for all (year, month) keys and remove year\n",
    "res = res.map(lambda l: (l[0][1], (1, l[1][1]/l[1][0])))\n",
    "\n",
    "# Reduce to sum up the values across all (month) keys\n",
    "res = res.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "# Map to compute the averages for all (month) keys\n",
    "longterm = res.map(lambda l: (l[0], l[1][1]/l[1][0]))\n",
    "\n",
    "# # Preparing stations w.r.t temperature (focus ostergotland monthly averages 1950-2014) ---------\n",
    "\n",
    "# Import data as pyspark.rdd.RDD\n",
    "directory = \"/user/x_julki/data/temperature-readings.csv\"\n",
    "# directory = \"./input/temperature-readings_Ostergotland.csv\"\n",
    "res = sc.textFile(directory)\n",
    "\n",
    "# Map to split every line l by ;\n",
    "res = res.map(lambda l: l.split(\";\"))\n",
    "\n",
    "# Map to convert every line to tuples in the form ((year, month, day, stationnumber), (temperature))\n",
    "res = res.map(lambda l: ((int(l[1][0:4]), int(l[1][5:7]), int(l[1][8:10]), int(l[0])), (float(l[3]), float(l[3]))))\n",
    "\n",
    "# Filter so that we only have Ostergotland stations and only measurements for 1950 to 2014\n",
    "res = res.filter(lambda l: l[0][3] in stat and l[0][0] >= 1950 and l[0][0] <= 2014)\n",
    "\n",
    "# Reduce to get the min and max values for all (year, month, day, stationnumber) keys\n",
    "res = res.reduceByKey(lambda x, y: (max(x[0], y[0]), min(x[1], y[1])))\n",
    "\n",
    "# Map to sum the min and max values across all (year, month, day, stationnumber) keys, and remove day\n",
    "res = res.map(lambda l: ((l[0][0], l[0][1], l[0][3]), (2, sum(l[1]))))\n",
    "\n",
    "# Reduce to sum up the values across all (year, month, stationnumber) keys\n",
    "res = res.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "# Map to compute the averages for all (year, month, stationnumber) keys, and remove stationnumber\n",
    "res = res.map(lambda l: ((l[0][0], l[0][1]), (1, l[1][1]/l[1][0])))\n",
    "\n",
    "# Reduce to sum up the values across all (year, month) keys\n",
    "res = res.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "# Map to compute the averages for all (year, month) keys and map to (month, (year, average))\n",
    "oster = res.map(lambda l: (l[0][1], (l[0][0], l[1][1]/l[1][0])))\n",
    "\n",
    "# Creating final, joined output ---------------------------------------------------------------\n",
    "\n",
    "# Join the RDDs\n",
    "res = oster.join(longterm)\n",
    "\n",
    "# Create ((year, month), difference) RDD\n",
    "res = res.map(lambda l: ((l[1][0][0], l[0]), (l[1][0][1] - l[1][1])))\n",
    "\n",
    "# Sort by year and month (not required but nice for output)\n",
    "res = res.sortByKey(ascending = False, numPartitions = 1)\n",
    "\n",
    "res.saveAsTextFile(\"BDA1_results/Asg6_withSpark_temperature-readings_stations-Ostergotland.txt\")\n",
    "# res.saveAsTextFile(\"BDA1_results_100000/Asg5_withSpark_temperature-readings_stations-Ostergotland.txt\")\n",
    "\n",
    "sc.stop()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, -6.737605911330049), (4, 2.9759027777777773), (6, 14.46111111111111), (8, 13.875741935483875), (10, 4.829714640198512), (12, -5.31681141439206), (9, 9.613205128205127), (11, -1.24474358974359), (1, -6.984967741935484), (3, -2.0228494623655915)]\n",
      "[(12, (1995, -5.170733712839975)), (12, (1996, -3.955831265508685)), (7, (1997, 18.038128806677193)), (9, (1997, 12.49307692307692)), (3, (1999, 1.2003870967741934)), (12, (1999, -1.5502481389578162)), (2, (2002, 1.6743506493506493)), (4, (2003, 3.8636363636363638)), (5, (2003, 10.589296187683285)), (4, (2004, 5.704545454545454))]\n",
      "[((2014, 12), 5.181034288292353), ((2014, 11), 5.625243589743589), ((2014, 10), 3.8971182043762678), ((2014, 9), 2.256340326340327), ((2014, 8), 1.5675131964809346), ((2014, 7), 3.6750322580645136), ((2014, 6), -0.688080808080807), ((2014, 5), 1.1678360215053747), ((2014, 4), 3.568763888888889), ((2014, 3), 5.854462365591397)]\n"
     ]
    }
   ],
   "source": [
    "# import pyspark\n",
    "\n",
    "# # Start SparkContext\n",
    "# sc = pyspark.SparkContext(appName=\"BDA\")\n",
    "\n",
    "# # Preparing stations w.r.t Ostergotland list ----------------------------------------------------\n",
    "\n",
    "# # Import data as pyspark.rdd.RDD\n",
    "# # directory = \"/user/x_julki/data/stations-Ostergotland.csv\"\n",
    "# directory = \"./input/stations-Ostergotland.csv\"\n",
    "# res = sc.textFile(directory)\n",
    "\n",
    "# # Split every line l by ;\n",
    "# res = res.map(lambda l: l.split(\";\"))\n",
    "\n",
    "# # Obtain a set with all station numbers from \n",
    "# stat = res.map(lambda l: (int(l[0])))\n",
    "# stat = set(stat.collect())\n",
    "\n",
    "# # Preparing stations w.r.t temperature (focus long-term monthly averages 1950-1980) -------------\n",
    "\n",
    "# # Import data as pyspark.rdd.RDD\n",
    "# # directory = \"/user/x_julki/data/temperature-readings.csv\"\n",
    "# directory = \"./input/temperature-readings_100000.csv\"\n",
    "# res = sc.textFile(directory)\n",
    "\n",
    "# # Map to split every line l by ;\n",
    "# res = res.map(lambda l: l.split(\";\"))\n",
    "\n",
    "# # Map to convert every line to tuples in the form ((year, month, day, stationnumber), (temperature, temperature))\n",
    "# res = res.map(lambda l: ((int(l[1][0:4]), int(l[1][5:7]), int(l[1][8:10]), int(l[0])), (float(l[3]), float(l[3]))))\n",
    "\n",
    "# # Filter so that we only have measurements for 1950 to 1980\n",
    "# res = res.filter(lambda l: l[0][0] >= 1950 and l[0][0] <= 1980)\n",
    "\n",
    "# # Reduce to get the min and max values for all (year, month, day, stationnumber) keys\n",
    "# res = res.reduceByKey(lambda x, y: (max(x[0], y[0]), min(x[1], y[1])))\n",
    "\n",
    "# # Map to sum the min and max values across all (year, month, day, stationnumber) keys, and remove day\n",
    "# res = res.map(lambda l: ((l[0][0], l[0][1], l[0][3]), (2, sum(l[1]))))\n",
    "\n",
    "# # Reduce to sum up the values across all (year, month, stationnumber) keys\n",
    "# res = res.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "# # Map to compute the averages for all (year, month, stationnumber) keys, and remove stationnumber\n",
    "# res = res.map(lambda l: ((l[0][0], l[0][1]), (1, l[1][1]/l[1][0])))\n",
    "\n",
    "# # Reduce to sum up the values across all (year, month) keys\n",
    "# res = res.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "# # Map to compute the averages for all (year, month) keys and remove year\n",
    "# res = res.map(lambda l: (l[0][1], (1, l[1][1]/l[1][0])))\n",
    "\n",
    "# # Reduce to sum up the values across all (month) keys\n",
    "# res = res.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "# # Map to compute the averages for all (month) keys\n",
    "# longterm = res.map(lambda l: (l[0], l[1][1]/l[1][0]))\n",
    "\n",
    "# print(longterm.collect()[0:10])\n",
    "\n",
    "# # # Preparing stations w.r.t temperature (focus ostergotland monthly averages 1950-2014) ---------\n",
    "\n",
    "# # Import data as pyspark.rdd.RDD\n",
    "# # directory = \"/user/x_julki/data/temperature-readings.csv\"\n",
    "# directory = \"./input/temperature-readings_Ostergotland.csv\"\n",
    "# res = sc.textFile(directory)\n",
    "\n",
    "# # Map to split every line l by ;\n",
    "# res = res.map(lambda l: l.split(\";\"))\n",
    "\n",
    "# # Map to convert every line to tuples in the form ((year, month, day, stationnumber), (temperature))\n",
    "# res = res.map(lambda l: ((int(l[1][0:4]), int(l[1][5:7]), int(l[1][8:10]), int(l[0])), (float(l[3]), float(l[3]))))\n",
    "\n",
    "# # Filter so that we only have Ostergotland stations and only measurements for 1950 to 2014\n",
    "# res = res.filter(lambda l: l[0][3] in stat and l[0][0] >= 1950 and l[0][0] <= 2014)\n",
    "\n",
    "# # Reduce to get the min and max values for all (year, month, day, stationnumber) keys\n",
    "# res = res.reduceByKey(lambda x, y: (max(x[0], y[0]), min(x[1], y[1])))\n",
    "\n",
    "# # Map to sum the min and max values across all (year, month, day, stationnumber) keys, and remove day\n",
    "# res = res.map(lambda l: ((l[0][0], l[0][1], l[0][3]), (2, sum(l[1]))))\n",
    "\n",
    "# # Reduce to sum up the values across all (year, month, stationnumber) keys\n",
    "# res = res.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "# # Map to compute the averages for all (year, month, stationnumber) keys, and remove stationnumber\n",
    "# res = res.map(lambda l: ((l[0][0], l[0][1]), (1, l[1][1]/l[1][0])))\n",
    "\n",
    "# # Reduce to sum up the values across all (year, month) keys\n",
    "# res = res.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "# # Map to compute the averages for all (year, month) keys\n",
    "# oster = res.map(lambda l: (l[0][1], (l[0][0], l[1][1]/l[1][0])))\n",
    "\n",
    "# print(oster.collect()[0:10])\n",
    "\n",
    "# # Creating final, joined output ---------------------------------------------------------------\n",
    "\n",
    "# # Join the RDDs\n",
    "# res = oster.join(longterm)\n",
    "\n",
    "# # Create ((year, month), difference) RDD\n",
    "# res = res.map(lambda l: ((l[1][0][0], l[0]), (l[1][0][1] - l[1][1])))\n",
    "\n",
    "# # Sort by year and month (not required but nice for output)\n",
    "# res = res.sortByKey(ascending = False, numPartitions = 1)\n",
    "\n",
    "# print(res.collect()[0:10])\n",
    "# # res.saveAsTextFile(\"BDA1_results/Asg6_withSpark_temperature-readings_stations-Ostergotland.txt\")\n",
    "# # # res.saveAsTextFile(\"BDA1_results_100000/Asg5_withSpark_temperature-readings_stations-Ostergotland.txt\")\n",
    "\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year Month            Difference\n",
      "0  2014    12    3.4830241827003467\n",
      "1  2014    11     4.426540558294422\n",
      "2  2014    10     3.344695229519207\n",
      "3  2014     9    1.5633300630200626\n",
      "4  2014     8     0.740400521225764\n",
      "5  2014     7    3.3235768010844353\n",
      "6  2014     6  -0.27220691081456394\n",
      "7  2014     5     1.915015553403249\n",
      "8  2014     4    3.7275705472342437\n",
      "9  2014     3     6.084907774499493\n",
      "...\n",
      "     Year Month          Difference\n",
      "770  1950    10  1.3622172236541044\n",
      "771  1950     9  1.8454512751412722\n",
      "772  1950     8   1.624564744099665\n",
      "773  1950     7  -0.259487715044596\n",
      "774  1950     6  1.3189294528217985\n",
      "775  1950     5   2.630176843725831\n",
      "776  1950     4   3.262070547234244\n",
      "777  1950     3  4.0903916454672355\n",
      "778  1950     2   4.439573057233748\n",
      "779  1950     1  0.6816929089364949\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "path = \"output/BDA1_results_final/Asg6_withSpark_temperature-readings_stations-Ostergotland.txt/part-00000\"\n",
    "colnames = [\"Year\", \"Month\", \"Difference\"]\n",
    "\n",
    "print_result(path, colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to obtain all temperatures for the Ostergotland stations (for local testing)\n",
    "# directory = \"./input/temperature-readings.csv\"\n",
    "# df = pd.read_csv(directory, sep = \";\", header = None)\n",
    "# df.columns = [\"station\", \"date\", \"time\", \"val\", \"other\"]\n",
    "# print(df.head())\n",
    "\n",
    "# directory = \"./input/stations-Ostergotland.csv\"\n",
    "# stat = pd.read_csv(directory, sep = \";\", header = None)\n",
    "# stat = set(stat.iloc[:,0])\n",
    "# print(len(stat))\n",
    "\n",
    "# df_sub = df[df['station'].isin(stat)]\n",
    "# df_sub.to_csv(\"./input/temperature-readings_Ostergotland.csv\", header = False, sep = \";\", index = False)\n",
    "# print(df_sub.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
